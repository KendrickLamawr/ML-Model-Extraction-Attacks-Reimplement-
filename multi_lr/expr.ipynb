{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can Luigi Mangione be super duper fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\.conda\\envs\\myenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "# Prepare the data\n",
    "X = iris.data  # Feature matrix\n",
    "y = iris.target  # Target vector\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use softmax regression (multi-class logistic regression)\n",
    "target_model = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=200)\n",
    "target_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = target_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "accuracy, y_pred[:5]  # Show accuracy and first 5 predictions\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# target_model.predict_log_proba(X_test)\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_test = encoder.fit_transform(pd.DataFrame(y_test))\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 4), (30, 3))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "def multinomial_loss(W, X, y, lambda_reg):\n",
    "    W = W.reshape(X.shape[1], -1)\n",
    "    epsilon = 1e-6\n",
    "    p_hat = softmax(X @ W)\n",
    "    loss = -np.mean(np.sum(np.log(p_hat + epsilon) * y,axis=1)) + .5 * lambda_reg * np.sum(W**2)\n",
    "    return loss\n",
    "def multinomial_grad(W, X, y, lambda_reg):\n",
    "    W = W.reshape(X.shape[1], -1)\n",
    "    cost = softmax(X @ W) - y\n",
    "    gradient = 1/X.shape[0] * X.T @ cost + lambda_reg * W\n",
    "    gradient = gradient.reshape(-1)\n",
    "    return gradient\n",
    "\n",
    "def find_score(W, X):\n",
    "    W.reshape(X.shape[1], -1)\n",
    "    p_hat = softmax(X @ W)\n",
    "    return np.argmax(p_hat, axis=1)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.640252825158451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 4.26620038, -1.79629018, -2.45492655,  1.91577184, -0.84647421,\n",
       "       -1.09461994,  3.35770367, -1.31010827, -2.04555733,  1.17620446,\n",
       "       -0.38955644, -0.75872396])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.random.randn(12)\n",
    "print(multinomial_loss(W, X_test, y_test, 0.01))\n",
    "multinomial_grad(W, X_test, y_test, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def run_opti(loss, gradient, X, Y, w_dim):\n",
    "    k = Y.shape[1] # Number of classes\n",
    "\n",
    "    best_w = None\n",
    "    best_acc = 0\n",
    "    num_classes = 3\n",
    "    acc = []\n",
    "    alphas = [10**x for x in range(-20, 4)] # regularization terms\n",
    "    fprimes = [gradient]\n",
    "\n",
    "    for fprime in fprimes:\n",
    "        for alpha in alphas:\n",
    "            w0 = 1e-8 * np.random.randn(X.shape[1] * num_classes)\n",
    "\n",
    "            num_unknows = len(w0.ravel())\n",
    "            method = \"BFGS\"\n",
    "            if num_unknows > 1000:\n",
    "                method = \"L-BFGS-B\"\n",
    "            # try:\n",
    "            optimLogitBFGS = minimize(loss, x0=w0,\n",
    "                                        method = method,\n",
    "                                        args = (X, Y, alpha),\n",
    "                                        jac = fprime,\n",
    "                                        options={'gtol': 1e-6,\n",
    "                                                'disp': True,\n",
    "                                                'maxiter': 100})\n",
    "            wopt = optimLogitBFGS.x\n",
    "            # print(multinomial_loss(wopt, X, Y, alpha))\n",
    "            # return wopt\n",
    "            wopt_reshape = wopt.reshape(X.shape[1], 3)\n",
    "            clonemodel_res = np.argmax(softmax(X @ wopt_reshape), axis = 1)\n",
    "            true_y = target_model.predict(X)\n",
    "            tempacc= (np.mean(true_y == clonemodel_res))\n",
    "            acc.append(tempacc)\n",
    "            # except ValueError:\n",
    "            #     print(f\"Failed to optimize with alpha={alpha} and method={method}\")\n",
    "            #     wopt = np.zeros(w0.shape)\n",
    "    return acc\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -0.000001\n",
      "         Iterations: 31\n",
      "         Function evaluations: 33\n",
      "         Gradient evaluations: 33\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.000001\n",
      "         Iterations: 31\n",
      "         Function evaluations: 33\n",
      "         Gradient evaluations: 33\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.000001\n",
      "         Iterations: 31\n",
      "         Function evaluations: 33\n",
      "         Gradient evaluations: 33\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.000001\n",
      "         Iterations: 31\n",
      "         Function evaluations: 33\n",
      "         Gradient evaluations: 33\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.000001\n",
      "         Iterations: 31\n",
      "         Function evaluations: 33\n",
      "         Gradient evaluations: 33\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.000001\n",
      "         Iterations: 31\n",
      "         Function evaluations: 33\n",
      "         Gradient evaluations: 33\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.000001\n",
      "         Iterations: 31\n",
      "         Function evaluations: 33\n",
      "         Gradient evaluations: 33\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.000001\n",
      "         Iterations: 31\n",
      "         Function evaluations: 33\n",
      "         Gradient evaluations: 33\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.000001\n",
      "         Iterations: 31\n",
      "         Function evaluations: 33\n",
      "         Gradient evaluations: 33\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.000001\n",
      "         Iterations: 31\n",
      "         Function evaluations: 33\n",
      "         Gradient evaluations: 33\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000001\n",
      "         Iterations: 47\n",
      "         Function evaluations: 50\n",
      "         Gradient evaluations: 50\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000009\n",
      "         Iterations: 51\n",
      "         Function evaluations: 53\n",
      "         Gradient evaluations: 53\n",
      "         Current function value: 0.000053\n",
      "         Iterations: 100\n",
      "         Function evaluations: 107\n",
      "         Gradient evaluations: 107\n",
      "         Current function value: 0.000321\n",
      "         Iterations: 100\n",
      "         Function evaluations: 108\n",
      "         Gradient evaluations: 108\n",
      "         Current function value: 0.001543\n",
      "         Iterations: 100\n",
      "         Function evaluations: 107\n",
      "         Gradient evaluations: 107\n",
      "         Current function value: 0.007781\n",
      "         Iterations: 100\n",
      "         Function evaluations: 107\n",
      "         Gradient evaluations: 107\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.032705\n",
      "         Iterations: 89\n",
      "         Function evaluations: 93\n",
      "         Gradient evaluations: 93\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.107235\n",
      "         Iterations: 65\n",
      "         Function evaluations: 69\n",
      "         Gradient evaluations: 69\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.279921\n",
      "         Iterations: 43\n",
      "         Function evaluations: 45\n",
      "         Gradient evaluations: 45\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.560883\n",
      "         Iterations: 22\n",
      "         Function evaluations: 24\n",
      "         Gradient evaluations: 24\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.872055\n",
      "         Iterations: 10\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.048149\n",
      "         Iterations: 11\n",
      "         Function evaluations: 18\n",
      "         Gradient evaluations: 18\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.090227\n",
      "         Iterations: 21\n",
      "         Function evaluations: 33\n",
      "         Gradient evaluations: 33\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.097672\n",
      "         Iterations: 17\n",
      "         Function evaluations: 32\n",
      "         Gradient evaluations: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python312\\site-packages\\scipy\\optimize\\_minimize.py:726: OptimizeWarning: Maximum number of iterations has been exceeded.\n",
      "  res = _minimize_bfgs(fun, x0, args, jac, callback, **options)\n",
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python312\\site-packages\\scipy\\optimize\\_minimize.py:726: OptimizeWarning: Maximum number of iterations has been exceeded.\n",
      "  res = _minimize_bfgs(fun, x0, args, jac, callback, **options)\n",
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python312\\site-packages\\scipy\\optimize\\_minimize.py:726: OptimizeWarning: Maximum number of iterations has been exceeded.\n",
      "  res = _minimize_bfgs(fun, x0, args, jac, callback, **options)\n",
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python312\\site-packages\\scipy\\optimize\\_minimize.py:726: OptimizeWarning: Maximum number of iterations has been exceeded.\n",
      "  res = _minimize_bfgs(fun, x0, args, jac, callback, **options)\n"
     ]
    }
   ],
   "source": [
    "accs= run_opti(multinomial_loss, multinomial_grad, X_test, y_test, (X_test.shape[1], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.8666666666666667,\n",
       " 0.7,\n",
       " 0.36666666666666664,\n",
       " 0.36666666666666664,\n",
       " 0.36666666666666664]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 2 1 0 0 0 2 1 1 0 0 1 2 2 1 2 1 2 2 0 2 1 0 0 0 1 2 0 0 0 1 0 1\n",
      " 2 0 1 2 0 2 2 1 2 2 1 0 1 2 0 0 1 1 0 2 0 0 2 1 2 2 2 2 1 0 0 2 2 0 0 0 1\n",
      " 2 0 2 2 0 1 1 2 1 2 0 2 1 2 1 1 1 1 1 1 0 1 2 2 0 1 2 1 0 2 0 1 2 2 1 2 1\n",
      " 1 2 2 0 1 2 0 1 2]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotypred = OneHotEncoder(sparse_output=False).fit_transform(pd.DataFrame(clone))\n",
    "\n",
    "true_y = target_model.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9583333333333334"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(true_y == clone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimize weights (binary case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomial_loss(W, X, y, lambda_reg):\n",
    "    epsilon = 1e-6\n",
    "    p_hat = softmax(X @ W)\n",
    "    loss = -np.mean(np.sum(np.log(p_hat + epsilon) * y,axis=1)) + .5 * lambda_reg * np.sum(W**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomial_loss2(W, X, y, lambda_reg):\n",
    "    p_hat = softmax(X @ W_init)\n",
    "    epsilon = 1e-6\n",
    "    return np.mean(np.log(p_hat*epsilon)*y) + .5 * lambda_reg * np.sum(W**2)\n",
    "\n",
    "def multinomial_grad2(W, X, y, lambda_reg):\n",
    "    cost = softmax(X @ W) - y\n",
    "    gradient = 1/X.shape[0] * X.T @ cost + lambda_reg * W\n",
    "    return gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-15.405764864688257"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_loss2(W_init, X, y, lambda_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.37536781, -0.3225786 , -0.33816599, -0.37609352])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_grad(W_init, X, y, lambda_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = minimize(\n",
    "    fun=multinomial_loss2,\n",
    "    x0=W_init,\n",
    "    args=(X, y, lambda_reg),\n",
    "    method=\"L-BFGS-B\",\n",
    "    jac=multinomial_grad2,\n",
    "    options={\"disp\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Weight Matrix (W):\n",
      "[[ 0.16171177 -0.13798981 -0.02372588]\n",
      " [ 0.23969347 -0.06567177 -0.17402611]\n",
      " [-0.15554053  0.08163731  0.07390005]\n",
      " [ 0.10807501 -0.07850352 -0.02957389]\n",
      " [-0.05274769 -0.01572479  0.06846955]]\n",
      "\n",
      "Final Loss: 1.0781224178155708\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Define the multinomial loss\n",
    "def multinomial_loss(W, X, y, lambda_reg):\n",
    "    epsilon = 1e-6\n",
    "    W = W.reshape(X.shape[1], -1)  # Reshape W to correct dimensions\n",
    "    p_hat = softmax(X @ W, axis=1)\n",
    "    loss = -np.mean(np.sum(np.log(p_hat + epsilon) * y, axis=1)) + 0.5 * lambda_reg * np.sum(W**2)\n",
    "    return loss\n",
    "\n",
    "# Define the gradient of the multinomial loss\n",
    "def multinomial_grad(W, X, y, lambda_reg):\n",
    "    W = W.reshape(X.shape[1], -1)  # Reshape W to correct dimensions\n",
    "    cost = softmax(X @ W, axis=1) - y\n",
    "    grad = (1 / X.shape[0]) * X.T @ cost + lambda_reg * W\n",
    "    return grad.flatten()  # Return as a flat array for optimizer compatibility\n",
    "\n",
    "# Simulated data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "n_features = 5\n",
    "n_classes = 3\n",
    "X = np.random.rand(n_samples, n_features)\n",
    "y = np.eye(n_classes)[np.random.choice(n_classes, n_samples)]  # One-hot encoded labels\n",
    "\n",
    "# Regularization parameter\n",
    "lambda_reg = 0.1\n",
    "\n",
    "# Initial guess for weights\n",
    "W_init = np.random.rand(n_features, n_classes).flatten()\n",
    "\n",
    "# Minimize the loss function\n",
    "result = minimize(\n",
    "    fun=multinomial_loss,\n",
    "    x0=W_init,\n",
    "    args=(X, y, lambda_reg),\n",
    "    method=\"L-BFGS-B\",\n",
    "    jac=multinomial_grad,\n",
    "    options={\"disp\": True}\n",
    ")\n",
    "\n",
    "# Extract the optimized weight matrix\n",
    "W_opt = result.x.reshape(n_features, n_classes)\n",
    "\n",
    "# Display results\n",
    "print(\"Optimal Weight Matrix (W):\")\n",
    "print(W_opt)\n",
    "print(\"\\nFinal Loss:\", result.fun)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0002774 ,  0.12586429,  0.09676574,  0.00332422,  0.10963249,\n",
       "        0.13713105,  0.02793491,  0.09639048,  0.05562939, -0.07426768,\n",
       "        0.14127733,  0.06947489, -0.00478114,  0.14959968,  0.02111559])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_grad(W_init, X, y, lambda_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def all_pairs(Y): #generate all pairs with different labels\n",
    "    \n",
    "    classes = pd.Series(Y).unique().tolist()\n",
    "    return [(i, j)\n",
    "            for i in range(len(Y))              # go over all points\n",
    "            for c in classes                    # and all other classes\n",
    "            if c != Y[i]\n",
    "            for j in np.where(Y == c)[0][0:1]   # and build a pair\n",
    "            if i > j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0),\n",
       " (2, 0),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (3, 2),\n",
       " (4, 0),\n",
       " (4, 2),\n",
       " (5, 0),\n",
       " (5, 2),\n",
       " (6, 0),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (7, 2)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pairs(np.array([1,2,0,1,2,2,0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  5.19615242, 15.58845727],\n",
       "       [ 5.19615242,  0.        , 10.39230485],\n",
       "       [15.58845727, 10.39230485,  0.        ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "Y = np.array([1,2,0,1,2,2,0,1])\n",
    "classes = pd.Series(Y).unique().tolist()\n",
    "\n",
    "np.where(Y == 2)\n",
    "squareform(pdist([[1,2,3],[4,5,6],[10,11,12]], 'euclidean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
